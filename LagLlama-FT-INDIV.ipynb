{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_567778/2384952011.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"lag-llama.ckpt\", map_location=device)\n",
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/fabric/utilities/cloud_io.py:56: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Forecasting batches: 100%|██████████| 1/1 [00:00<00:00, 12.46it/s]\n",
      "Ground truth: 100%|██████████| 1/1 [00:00<00:00, 4181.76it/s]\n",
      "/tmp/ipykernel_567778/2384952011.py:131: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"lag-llama.ckpt\", map_location=device)\n",
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/fabric/utilities/cloud_io.py:56: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name          | Type               | Params\n",
      "-----------------------------------------------------\n",
      "0 | model         | LagLlamaModel      | 2.4 M \n",
      "1 | augmentations | ApplyAugmentations | 0     \n",
      "-----------------------------------------------------\n",
      "2.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 M     Total params\n",
      "9.797     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: |          | 50/? [00:00<00:00, 57.36it/s, v_num=8166]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 50: 'train_loss' reached 7.79764 (best 7.79764), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8166/checkpoints/epoch=0-step=50.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: |          | 50/? [00:00<00:00, 61.95it/s, v_num=8166]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 100: 'train_loss' reached 6.90706 (best 6.90706), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8166/checkpoints/epoch=1-step=100.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: |          | 50/? [00:00<00:00, 59.90it/s, v_num=8166]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 150: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: |          | 50/? [00:00<00:00, 65.31it/s, v_num=8166]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 200: 'train_loss' reached 6.86396 (best 6.86396), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8166/checkpoints/epoch=3-step=200.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: |          | 50/? [00:00<00:00, 66.16it/s, v_num=8166]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 250: 'train_loss' reached 6.84568 (best 6.84568), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8166/checkpoints/epoch=4-step=250.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: |          | 50/? [00:00<00:00, 61.33it/s, v_num=8166]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 300: 'train_loss' reached 6.63281 (best 6.63281), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8166/checkpoints/epoch=5-step=300.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: |          | 50/? [00:00<00:00, 59.79it/s, v_num=8166]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 350: 'train_loss' reached 6.44036 (best 6.44036), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8166/checkpoints/epoch=6-step=350.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: |          | 50/? [00:00<00:00, 63.46it/s, v_num=8166]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 400: 'train_loss' reached 6.29532 (best 6.29532), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8166/checkpoints/epoch=7-step=400.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: |          | 50/? [00:00<00:00, 65.15it/s, v_num=8166]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 450: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: |          | 50/? [00:00<00:00, 62.64it/s, v_num=8166]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 500: 'train_loss' reached 6.17848 (best 6.17848), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8166/checkpoints/epoch=9-step=500.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: |          | 50/? [00:00<00:00, 62.64it/s, v_num=8166]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 550: 'train_loss' reached 6.03073 (best 6.03073), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8166/checkpoints/epoch=10-step=550.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: |          | 50/? [00:00<00:00, 68.48it/s, v_num=8166]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 600: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: |          | 50/? [00:00<00:00, 67.10it/s, v_num=8166]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 650: 'train_loss' reached 5.95717 (best 5.95717), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8166/checkpoints/epoch=12-step=650.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: |          | 50/? [00:00<00:00, 62.22it/s, v_num=8166]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 700: 'train_loss' reached 5.88034 (best 5.88034), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8166/checkpoints/epoch=13-step=700.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: |          | 50/? [00:00<00:00, 68.11it/s, v_num=8166]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 750: 'train_loss' reached 5.84473 (best 5.84473), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8166/checkpoints/epoch=14-step=750.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: |          | 50/? [00:00<00:00, 66.03it/s, v_num=8166]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 800: 'train_loss' reached 5.78027 (best 5.78027), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8166/checkpoints/epoch=15-step=800.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: |          | 50/? [00:00<00:00, 67.26it/s, v_num=8166]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 850: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: |          | 50/? [00:00<00:00, 67.50it/s, v_num=8166]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 900: 'train_loss' reached 5.73208 (best 5.73208), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8166/checkpoints/epoch=17-step=900.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: |          | 50/? [00:00<00:00, 68.57it/s, v_num=8166]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 950: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: |          | 50/? [00:00<00:00, 70.13it/s, v_num=8166]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 1000: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: |          | 50/? [00:00<00:00, 69.22it/s, v_num=8166]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, global step 1050: 'train_loss' reached 5.68953 (best 5.68953), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8166/checkpoints/epoch=20-step=1050.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: |          | 50/? [00:00<00:00, 69.62it/s, v_num=8166]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, global step 1100: 'train_loss' reached 5.67951 (best 5.67951), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8166/checkpoints/epoch=21-step=1100.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: |          | 50/? [00:00<00:00, 67.65it/s, v_num=8166]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22, global step 1150: 'train_loss' reached 5.66262 (best 5.66262), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8166/checkpoints/epoch=22-step=1150.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: |          | 50/? [00:00<00:00, 68.21it/s, v_num=8166]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23, global step 1200: 'train_loss' reached 5.57638 (best 5.57638), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8166/checkpoints/epoch=23-step=1200.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: |          | 50/? [00:00<00:00, 67.40it/s, v_num=8166]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24, global step 1250: 'train_loss' reached 5.53686 (best 5.53686), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8166/checkpoints/epoch=24-step=1250.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: |          | 50/? [00:00<00:00, 67.25it/s, v_num=8166]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25, global step 1300: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: |          | 50/? [00:00<00:00, 69.07it/s, v_num=8166]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26, global step 1350: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: |          | 50/? [00:00<00:00, 70.74it/s, v_num=8166]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27, global step 1400: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: |          | 50/? [00:00<00:00, 66.72it/s, v_num=8166]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28, global step 1450: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: |          | 50/? [00:00<00:00, 67.01it/s, v_num=8166]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: |          | 50/? [00:00<00:00, 66.08it/s, v_num=8166]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/fabric/utilities/cloud_io.py:56: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Forecasting batches: 100%|██████████| 1/1 [00:00<00:00, 13.77it/s]\n",
      "Ground truth: 100%|██████████| 1/1 [00:00<00:00, 5133.79it/s]\n",
      "/tmp/ipykernel_567778/2384952011.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"lag-llama.ckpt\", map_location=device)\n",
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/fabric/utilities/cloud_io.py:56: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Forecasting batches: 100%|██████████| 1/1 [00:00<00:00, 12.93it/s]\n",
      "Ground truth: 100%|██████████| 1/1 [00:00<00:00, 3964.37it/s]\n",
      "/tmp/ipykernel_567778/2384952011.py:131: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"lag-llama.ckpt\", map_location=device)\n",
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/fabric/utilities/cloud_io.py:56: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name          | Type               | Params\n",
      "-----------------------------------------------------\n",
      "0 | model         | LagLlamaModel      | 2.4 M \n",
      "1 | augmentations | ApplyAugmentations | 0     \n",
      "-----------------------------------------------------\n",
      "2.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 M     Total params\n",
      "9.797     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: |          | 50/? [00:00<00:00, 61.22it/s, v_num=8167]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 50: 'train_loss' reached 9.94364 (best 9.94364), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8167/checkpoints/epoch=0-step=50.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: |          | 50/? [00:00<00:00, 61.95it/s, v_num=8167]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 100: 'train_loss' reached 9.06833 (best 9.06833), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8167/checkpoints/epoch=1-step=100.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: |          | 50/? [00:00<00:00, 64.00it/s, v_num=8167]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 150: 'train_loss' reached 8.98982 (best 8.98982), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8167/checkpoints/epoch=2-step=150.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: |          | 50/? [00:00<00:00, 65.44it/s, v_num=8167]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 200: 'train_loss' reached 8.64536 (best 8.64536), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8167/checkpoints/epoch=3-step=200.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: |          | 50/? [00:00<00:00, 68.70it/s, v_num=8167]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 250: 'train_loss' reached 8.43111 (best 8.43111), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8167/checkpoints/epoch=4-step=250.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: |          | 50/? [00:00<00:00, 67.74it/s, v_num=8167]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 300: 'train_loss' reached 8.31400 (best 8.31400), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8167/checkpoints/epoch=5-step=300.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: |          | 50/? [00:00<00:00, 67.78it/s, v_num=8167]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 350: 'train_loss' reached 8.17244 (best 8.17244), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8167/checkpoints/epoch=6-step=350.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: |          | 50/? [00:00<00:00, 66.54it/s, v_num=8167]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 400: 'train_loss' reached 8.09966 (best 8.09966), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8167/checkpoints/epoch=7-step=400.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: |          | 50/? [00:00<00:00, 62.11it/s, v_num=8167]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 450: 'train_loss' reached 8.03230 (best 8.03230), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8167/checkpoints/epoch=8-step=450.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: |          | 50/? [00:00<00:00, 67.57it/s, v_num=8167]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 500: 'train_loss' reached 7.90239 (best 7.90239), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8167/checkpoints/epoch=9-step=500.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: |          | 50/? [00:00<00:00, 68.56it/s, v_num=8167]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 550: 'train_loss' reached 7.81655 (best 7.81655), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8167/checkpoints/epoch=10-step=550.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: |          | 50/? [00:00<00:00, 66.62it/s, v_num=8167]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 600: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: |          | 50/? [00:00<00:00, 56.42it/s, v_num=8167]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 650: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: |          | 50/? [00:00<00:00, 66.05it/s, v_num=8167]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 700: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: |          | 50/? [00:00<00:00, 69.75it/s, v_num=8167]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 750: 'train_loss' reached 7.66246 (best 7.66246), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8167/checkpoints/epoch=14-step=750.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: |          | 50/? [00:00<00:00, 69.93it/s, v_num=8167]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 800: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: |          | 50/? [00:00<00:00, 69.15it/s, v_num=8167]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 850: 'train_loss' reached 7.62236 (best 7.62236), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8167/checkpoints/epoch=16-step=850.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: |          | 50/? [00:00<00:00, 68.57it/s, v_num=8167]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 900: 'train_loss' reached 7.60237 (best 7.60237), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8167/checkpoints/epoch=17-step=900.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: |          | 50/? [00:00<00:00, 69.63it/s, v_num=8167]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 950: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: |          | 50/? [00:00<00:00, 67.39it/s, v_num=8167]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 1000: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: |          | 50/? [00:00<00:00, 68.62it/s, v_num=8167]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, global step 1050: 'train_loss' reached 7.54363 (best 7.54363), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8167/checkpoints/epoch=20-step=1050.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: |          | 50/? [00:00<00:00, 65.45it/s, v_num=8167]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, global step 1100: 'train_loss' reached 7.54136 (best 7.54136), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8167/checkpoints/epoch=21-step=1100.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: |          | 50/? [00:00<00:00, 66.09it/s, v_num=8167]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22, global step 1150: 'train_loss' reached 7.53172 (best 7.53172), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8167/checkpoints/epoch=22-step=1150.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: |          | 50/? [00:00<00:00, 68.30it/s, v_num=8167]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23, global step 1200: 'train_loss' reached 7.42192 (best 7.42192), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8167/checkpoints/epoch=23-step=1200.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: |          | 50/? [00:00<00:00, 68.99it/s, v_num=8167]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24, global step 1250: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: |          | 50/? [00:00<00:00, 68.95it/s, v_num=8167]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25, global step 1300: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: |          | 50/? [00:00<00:00, 69.07it/s, v_num=8167]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26, global step 1350: 'train_loss' reached 7.38905 (best 7.38905), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8167/checkpoints/epoch=26-step=1350.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: |          | 50/? [00:00<00:00, 69.79it/s, v_num=8167]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27, global step 1400: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: |          | 50/? [00:00<00:00, 71.81it/s, v_num=8167]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28, global step 1450: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: |          | 50/? [00:00<00:00, 70.41it/s, v_num=8167]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29, global step 1500: 'train_loss' reached 7.37815 (best 7.37815), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8167/checkpoints/epoch=29-step=1500.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: |          | 50/? [00:00<00:00, 66.14it/s, v_num=8167]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/fabric/utilities/cloud_io.py:56: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Forecasting batches: 100%|██████████| 1/1 [00:00<00:00, 13.78it/s]\n",
      "Ground truth: 100%|██████████| 1/1 [00:00<00:00, 3132.42it/s]\n",
      "/tmp/ipykernel_567778/2384952011.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"lag-llama.ckpt\", map_location=device)\n",
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/fabric/utilities/cloud_io.py:56: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Forecasting batches: 100%|██████████| 1/1 [00:00<00:00, 13.27it/s]\n",
      "Ground truth: 100%|██████████| 1/1 [00:00<00:00, 5377.31it/s]\n",
      "/tmp/ipykernel_567778/2384952011.py:131: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"lag-llama.ckpt\", map_location=device)\n",
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/fabric/utilities/cloud_io.py:56: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name          | Type               | Params\n",
      "-----------------------------------------------------\n",
      "0 | model         | LagLlamaModel      | 2.4 M \n",
      "1 | augmentations | ApplyAugmentations | 0     \n",
      "-----------------------------------------------------\n",
      "2.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 M     Total params\n",
      "9.797     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: |          | 50/? [00:00<00:00, 59.22it/s, v_num=8168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 50: 'train_loss' reached 9.60576 (best 9.60576), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8168/checkpoints/epoch=0-step=50.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: |          | 50/? [00:00<00:00, 61.50it/s, v_num=8168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 100: 'train_loss' reached 8.94904 (best 8.94904), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8168/checkpoints/epoch=1-step=100.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: |          | 50/? [00:00<00:00, 66.47it/s, v_num=8168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 150: 'train_loss' reached 8.34433 (best 8.34433), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8168/checkpoints/epoch=2-step=150.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: |          | 50/? [00:00<00:00, 61.96it/s, v_num=8168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 200: 'train_loss' reached 8.21906 (best 8.21906), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8168/checkpoints/epoch=3-step=200.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: |          | 50/? [00:00<00:00, 59.17it/s, v_num=8168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 250: 'train_loss' reached 8.20238 (best 8.20238), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8168/checkpoints/epoch=4-step=250.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: |          | 50/? [00:00<00:00, 63.46it/s, v_num=8168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 300: 'train_loss' reached 8.06697 (best 8.06697), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8168/checkpoints/epoch=5-step=300.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: |          | 50/? [00:00<00:00, 63.19it/s, v_num=8168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 350: 'train_loss' reached 7.75115 (best 7.75115), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8168/checkpoints/epoch=6-step=350.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: |          | 50/? [00:00<00:00, 61.07it/s, v_num=8168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 400: 'train_loss' reached 7.70657 (best 7.70657), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8168/checkpoints/epoch=7-step=400.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: |          | 50/? [00:00<00:00, 59.35it/s, v_num=8168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 450: 'train_loss' reached 7.67804 (best 7.67804), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8168/checkpoints/epoch=8-step=450.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: |          | 50/? [00:00<00:00, 63.47it/s, v_num=8168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 500: 'train_loss' reached 7.55765 (best 7.55765), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8168/checkpoints/epoch=9-step=500.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: |          | 50/? [00:00<00:00, 64.13it/s, v_num=8168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 550: 'train_loss' reached 7.52266 (best 7.52266), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8168/checkpoints/epoch=10-step=550.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: |          | 50/? [00:00<00:00, 67.08it/s, v_num=8168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 600: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: |          | 50/? [00:00<00:00, 64.50it/s, v_num=8168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 650: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: |          | 50/? [00:00<00:00, 64.90it/s, v_num=8168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 700: 'train_loss' reached 7.51109 (best 7.51109), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8168/checkpoints/epoch=13-step=700.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: |          | 50/? [00:00<00:00, 65.60it/s, v_num=8168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 750: 'train_loss' reached 7.33101 (best 7.33101), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8168/checkpoints/epoch=14-step=750.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: |          | 50/? [00:00<00:00, 67.57it/s, v_num=8168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 800: 'train_loss' reached 7.30506 (best 7.30506), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8168/checkpoints/epoch=15-step=800.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: |          | 50/? [00:00<00:00, 65.41it/s, v_num=8168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 850: 'train_loss' reached 7.26685 (best 7.26685), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8168/checkpoints/epoch=16-step=850.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: |          | 50/? [00:00<00:00, 68.24it/s, v_num=8168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 900: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: |          | 50/? [00:00<00:00, 69.74it/s, v_num=8168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 950: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: |          | 50/? [00:00<00:00, 69.53it/s, v_num=8168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 1000: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: |          | 50/? [00:00<00:00, 67.48it/s, v_num=8168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, global step 1050: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: |          | 50/? [00:00<00:00, 65.60it/s, v_num=8168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, global step 1100: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: |          | 50/? [00:00<00:00, 67.08it/s, v_num=8168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22, global step 1150: 'train_loss' reached 7.22488 (best 7.22488), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8168/checkpoints/epoch=22-step=1150.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: |          | 50/? [00:00<00:00, 59.52it/s, v_num=8168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23, global step 1200: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: |          | 50/? [00:00<00:00, 65.53it/s, v_num=8168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24, global step 1250: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: |          | 50/? [00:00<00:00, 67.47it/s, v_num=8168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25, global step 1300: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: |          | 50/? [00:00<00:00, 69.38it/s, v_num=8168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26, global step 1350: 'train_loss' reached 7.19518 (best 7.19518), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8168/checkpoints/epoch=26-step=1350.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: |          | 50/? [00:00<00:00, 70.42it/s, v_num=8168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27, global step 1400: 'train_loss' reached 7.16372 (best 7.16372), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8168/checkpoints/epoch=27-step=1400.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: |          | 50/? [00:00<00:00, 68.71it/s, v_num=8168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28, global step 1450: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: |          | 50/? [00:00<00:00, 69.61it/s, v_num=8168]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: |          | 50/? [00:00<00:00, 68.61it/s, v_num=8168]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/fabric/utilities/cloud_io.py:56: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Forecasting batches: 100%|██████████| 1/1 [00:00<00:00, 13.76it/s]\n",
      "Ground truth: 100%|██████████| 1/1 [00:00<00:00, 2465.79it/s]\n",
      "/tmp/ipykernel_567778/2384952011.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"lag-llama.ckpt\", map_location=device)\n",
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/fabric/utilities/cloud_io.py:56: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Forecasting batches: 100%|██████████| 1/1 [00:00<00:00, 13.22it/s]\n",
      "Ground truth: 100%|██████████| 1/1 [00:00<00:00, 4619.28it/s]\n",
      "/tmp/ipykernel_567778/2384952011.py:131: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"lag-llama.ckpt\", map_location=device)\n",
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/fabric/utilities/cloud_io.py:56: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name          | Type               | Params\n",
      "-----------------------------------------------------\n",
      "0 | model         | LagLlamaModel      | 2.4 M \n",
      "1 | augmentations | ApplyAugmentations | 0     \n",
      "-----------------------------------------------------\n",
      "2.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 M     Total params\n",
      "9.797     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: |          | 50/? [00:00<00:00, 58.00it/s, v_num=8169]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 50: 'train_loss' reached 7.13102 (best 7.13102), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8169/checkpoints/epoch=0-step=50.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: |          | 50/? [00:00<00:00, 55.70it/s, v_num=8169]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 100: 'train_loss' reached 6.73644 (best 6.73644), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8169/checkpoints/epoch=1-step=100.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: |          | 50/? [00:00<00:00, 59.19it/s, v_num=8169]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 150: 'train_loss' reached 6.57448 (best 6.57448), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8169/checkpoints/epoch=2-step=150.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: |          | 50/? [00:00<00:00, 57.61it/s, v_num=8169]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 200: 'train_loss' reached 6.07501 (best 6.07501), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8169/checkpoints/epoch=3-step=200.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: |          | 50/? [00:00<00:00, 60.51it/s, v_num=8169]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 250: 'train_loss' reached 5.80931 (best 5.80931), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8169/checkpoints/epoch=4-step=250.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: |          | 50/? [00:00<00:00, 59.82it/s, v_num=8169]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 300: 'train_loss' reached 5.64231 (best 5.64231), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8169/checkpoints/epoch=5-step=300.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: |          | 50/? [00:00<00:00, 61.38it/s, v_num=8169]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 350: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: |          | 50/? [00:00<00:00, 67.46it/s, v_num=8169]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 400: 'train_loss' reached 5.54451 (best 5.54451), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8169/checkpoints/epoch=7-step=400.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: |          | 50/? [00:00<00:00, 66.82it/s, v_num=8169]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 450: 'train_loss' reached 5.40241 (best 5.40241), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8169/checkpoints/epoch=8-step=450.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: |          | 50/? [00:00<00:00, 68.72it/s, v_num=8169]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 500: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: |          | 50/? [00:00<00:00, 69.37it/s, v_num=8169]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 550: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: |          | 50/? [00:00<00:00, 69.79it/s, v_num=8169]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 600: 'train_loss' reached 5.34039 (best 5.34039), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8169/checkpoints/epoch=11-step=600.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: |          | 50/? [00:00<00:00, 69.74it/s, v_num=8169]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 650: 'train_loss' reached 5.31491 (best 5.31491), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8169/checkpoints/epoch=12-step=650.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: |          | 50/? [00:00<00:00, 70.05it/s, v_num=8169]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 700: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: |          | 50/? [00:00<00:00, 61.72it/s, v_num=8169]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 750: 'train_loss' reached 5.26578 (best 5.26578), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8169/checkpoints/epoch=14-step=750.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: |          | 50/? [00:00<00:00, 59.51it/s, v_num=8169]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 800: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: |          | 50/? [00:00<00:00, 57.11it/s, v_num=8169]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 850: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: |          | 50/? [00:00<00:00, 59.97it/s, v_num=8169]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 900: 'train_loss' reached 5.11709 (best 5.11709), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8169/checkpoints/epoch=17-step=900.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: |          | 50/? [00:00<00:00, 59.42it/s, v_num=8169]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 950: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: |          | 50/? [00:00<00:00, 60.88it/s, v_num=8169]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 1000: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: |          | 50/? [00:00<00:00, 66.37it/s, v_num=8169]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, global step 1050: 'train_loss' reached 5.06698 (best 5.06698), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8169/checkpoints/epoch=20-step=1050.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: |          | 50/? [00:00<00:00, 66.81it/s, v_num=8169]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, global step 1100: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: |          | 50/? [00:00<00:00, 65.21it/s, v_num=8169]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22, global step 1150: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: |          | 50/? [00:00<00:00, 65.76it/s, v_num=8169]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23, global step 1200: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: |          | 50/? [00:00<00:00, 66.54it/s, v_num=8169]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24, global step 1250: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: |          | 50/? [00:00<00:00, 69.08it/s, v_num=8169]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25, global step 1300: 'train_loss' reached 5.00333 (best 5.00333), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8169/checkpoints/epoch=25-step=1300.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: |          | 50/? [00:00<00:00, 67.37it/s, v_num=8169]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26, global step 1350: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: |          | 50/? [00:00<00:00, 67.80it/s, v_num=8169]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27, global step 1400: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: |          | 50/? [00:00<00:00, 68.23it/s, v_num=8169]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28, global step 1450: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: |          | 50/? [00:00<00:00, 67.99it/s, v_num=8169]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: |          | 50/? [00:00<00:00, 67.04it/s, v_num=8169]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/fabric/utilities/cloud_io.py:56: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Forecasting batches: 100%|██████████| 1/1 [00:00<00:00, 13.61it/s]\n",
      "Ground truth: 100%|██████████| 1/1 [00:00<00:00, 4793.49it/s]\n",
      "/tmp/ipykernel_567778/2384952011.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"lag-llama.ckpt\", map_location=device)\n",
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/fabric/utilities/cloud_io.py:56: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Forecasting batches: 100%|██████████| 1/1 [00:00<00:00, 13.20it/s]\n",
      "Ground truth: 100%|██████████| 1/1 [00:00<00:00, 4346.43it/s]\n",
      "/tmp/ipykernel_567778/2384952011.py:131: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"lag-llama.ckpt\", map_location=device)\n",
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/fabric/utilities/cloud_io.py:56: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name          | Type               | Params\n",
      "-----------------------------------------------------\n",
      "0 | model         | LagLlamaModel      | 2.4 M \n",
      "1 | augmentations | ApplyAugmentations | 0     \n",
      "-----------------------------------------------------\n",
      "2.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 M     Total params\n",
      "9.797     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: |          | 50/? [00:00<00:00, 66.49it/s, v_num=8170]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 50: 'train_loss' reached 11.46507 (best 11.46507), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8170/checkpoints/epoch=0-step=50.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: |          | 50/? [00:00<00:00, 63.59it/s, v_num=8170]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 100: 'train_loss' reached 10.97566 (best 10.97566), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8170/checkpoints/epoch=1-step=100.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: |          | 50/? [00:00<00:00, 60.84it/s, v_num=8170]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 150: 'train_loss' reached 10.76421 (best 10.76421), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8170/checkpoints/epoch=2-step=150.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: |          | 8/? [00:00<00:00, 55.85it/s, v_num=8170] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/fabric/utilities/cloud_io.py:56: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Forecasting batches: 100%|██████████| 1/1 [00:00<00:00, 13.56it/s]\n",
      "Ground truth: 100%|██████████| 1/1 [00:00<00:00, 4495.50it/s]\n",
      "/tmp/ipykernel_567778/2384952011.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"lag-llama.ckpt\", map_location=device)\n",
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/fabric/utilities/cloud_io.py:56: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Forecasting batches: 100%|██████████| 1/1 [00:00<00:00, 12.92it/s]\n",
      "Ground truth: 100%|██████████| 1/1 [00:00<00:00, 3711.77it/s]\n",
      "/tmp/ipykernel_567778/2384952011.py:131: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"lag-llama.ckpt\", map_location=device)\n",
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/fabric/utilities/cloud_io.py:56: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name          | Type               | Params\n",
      "-----------------------------------------------------\n",
      "0 | model         | LagLlamaModel      | 2.4 M \n",
      "1 | augmentations | ApplyAugmentations | 0     \n",
      "-----------------------------------------------------\n",
      "2.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 M     Total params\n",
      "9.797     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: |          | 50/? [00:00<00:00, 58.25it/s, v_num=8171]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 50: 'train_loss' reached 10.20613 (best 10.20613), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8171/checkpoints/epoch=0-step=50.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: |          | 50/? [00:00<00:00, 62.04it/s, v_num=8171]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 100: 'train_loss' reached 9.77644 (best 9.77644), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8171/checkpoints/epoch=1-step=100.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: |          | 50/? [00:00<00:00, 65.10it/s, v_num=8171]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 150: 'train_loss' reached 9.72576 (best 9.72576), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8171/checkpoints/epoch=2-step=150.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: |          | 50/? [00:00<00:00, 65.85it/s, v_num=8171]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 200: 'train_loss' reached 9.34776 (best 9.34776), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8171/checkpoints/epoch=3-step=200.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: |          | 50/? [00:00<00:00, 65.76it/s, v_num=8171]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 250: 'train_loss' reached 9.11297 (best 9.11297), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8171/checkpoints/epoch=4-step=250.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: |          | 50/? [00:00<00:00, 65.10it/s, v_num=8171]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 300: 'train_loss' reached 8.90503 (best 8.90503), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8171/checkpoints/epoch=5-step=300.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: |          | 50/? [00:00<00:00, 67.06it/s, v_num=8171]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 350: 'train_loss' reached 8.79884 (best 8.79884), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8171/checkpoints/epoch=6-step=350.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: |          | 50/? [00:00<00:00, 67.56it/s, v_num=8171]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 400: 'train_loss' reached 8.68704 (best 8.68704), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8171/checkpoints/epoch=7-step=400.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: |          | 50/? [00:00<00:00, 66.65it/s, v_num=8171]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 450: 'train_loss' reached 8.56622 (best 8.56622), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8171/checkpoints/epoch=8-step=450.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: |          | 50/? [00:00<00:00, 63.50it/s, v_num=8171]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 500: 'train_loss' reached 8.49815 (best 8.49815), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8171/checkpoints/epoch=9-step=500.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: |          | 50/? [00:00<00:00, 58.59it/s, v_num=8171]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 550: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: |          | 50/? [00:00<00:00, 64.14it/s, v_num=8171]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 600: 'train_loss' reached 8.32580 (best 8.32580), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8171/checkpoints/epoch=11-step=600.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: |          | 50/? [00:00<00:00, 63.87it/s, v_num=8171]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 650: 'train_loss' reached 8.30827 (best 8.30827), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8171/checkpoints/epoch=12-step=650.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: |          | 50/? [00:00<00:00, 64.43it/s, v_num=8171]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 700: 'train_loss' reached 8.26345 (best 8.26345), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8171/checkpoints/epoch=13-step=700.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: |          | 50/? [00:00<00:00, 65.37it/s, v_num=8171]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 750: 'train_loss' reached 8.19531 (best 8.19531), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8171/checkpoints/epoch=14-step=750.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: |          | 50/? [00:00<00:00, 64.33it/s, v_num=8171]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 800: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: |          | 50/? [00:00<00:00, 64.04it/s, v_num=8171]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 850: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: |          | 50/? [00:00<00:00, 65.49it/s, v_num=8171]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 900: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: |          | 50/? [00:00<00:00, 65.73it/s, v_num=8171]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 950: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: |          | 50/? [00:00<00:00, 66.59it/s, v_num=8171]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 1000: 'train_loss' reached 8.14069 (best 8.14069), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8171/checkpoints/epoch=19-step=1000.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: |          | 50/? [00:00<00:00, 67.12it/s, v_num=8171]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, global step 1050: 'train_loss' reached 7.99560 (best 7.99560), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8171/checkpoints/epoch=20-step=1050.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: |          | 50/? [00:00<00:00, 64.92it/s, v_num=8171]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, global step 1100: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: |          | 50/? [00:00<00:00, 65.77it/s, v_num=8171]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22, global step 1150: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: |          | 50/? [00:00<00:00, 64.24it/s, v_num=8171]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23, global step 1200: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: |          | 50/? [00:00<00:00, 64.97it/s, v_num=8171]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24, global step 1250: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: |          | 50/? [00:00<00:00, 65.19it/s, v_num=8171]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25, global step 1300: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: |          | 50/? [00:00<00:00, 66.59it/s, v_num=8171]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26, global step 1350: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: |          | 50/? [00:00<00:00, 60.14it/s, v_num=8171]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27, global step 1400: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: |          | 50/? [00:00<00:00, 64.49it/s, v_num=8171]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28, global step 1450: 'train_loss' reached 7.88331 (best 7.88331), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8171/checkpoints/epoch=28-step=1450.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: |          | 50/? [00:00<00:00, 60.85it/s, v_num=8171]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: |          | 50/? [00:00<00:00, 60.05it/s, v_num=8171]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/fabric/utilities/cloud_io.py:56: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Forecasting batches: 100%|██████████| 1/1 [00:00<00:00, 13.66it/s]\n",
      "Ground truth: 100%|██████████| 1/1 [00:00<00:00, 4951.95it/s]\n",
      "/tmp/ipykernel_567778/2384952011.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"lag-llama.ckpt\", map_location=device)\n",
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/fabric/utilities/cloud_io.py:56: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Forecasting batches: 100%|██████████| 1/1 [00:00<00:00, 13.25it/s]\n",
      "Ground truth: 100%|██████████| 1/1 [00:00<00:00, 4364.52it/s]\n",
      "/tmp/ipykernel_567778/2384952011.py:131: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"lag-llama.ckpt\", map_location=device)\n",
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/fabric/utilities/cloud_io.py:56: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name          | Type               | Params\n",
      "-----------------------------------------------------\n",
      "0 | model         | LagLlamaModel      | 2.4 M \n",
      "1 | augmentations | ApplyAugmentations | 0     \n",
      "-----------------------------------------------------\n",
      "2.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 M     Total params\n",
      "9.797     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: |          | 50/? [00:00<00:00, 63.20it/s, v_num=8172]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 50: 'train_loss' reached 10.81384 (best 10.81384), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8172/checkpoints/epoch=0-step=50.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: |          | 50/? [00:00<00:00, 67.70it/s, v_num=8172]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 100: 'train_loss' reached 10.16265 (best 10.16265), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8172/checkpoints/epoch=1-step=100.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: |          | 50/? [00:00<00:00, 66.27it/s, v_num=8172]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 150: 'train_loss' reached 9.94147 (best 9.94147), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8172/checkpoints/epoch=2-step=150.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: |          | 50/? [00:00<00:00, 67.86it/s, v_num=8172]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 200: 'train_loss' reached 9.72832 (best 9.72832), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8172/checkpoints/epoch=3-step=200.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: |          | 50/? [00:00<00:00, 61.17it/s, v_num=8172]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 250: 'train_loss' reached 9.56677 (best 9.56677), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8172/checkpoints/epoch=4-step=250.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: |          | 50/? [00:00<00:00, 58.53it/s, v_num=8172]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 300: 'train_loss' reached 9.40046 (best 9.40046), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8172/checkpoints/epoch=5-step=300.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: |          | 50/? [00:00<00:00, 61.17it/s, v_num=8172]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 350: 'train_loss' reached 9.32689 (best 9.32689), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8172/checkpoints/epoch=6-step=350.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: |          | 50/? [00:00<00:00, 61.28it/s, v_num=8172]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 400: 'train_loss' reached 9.15421 (best 9.15421), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8172/checkpoints/epoch=7-step=400.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: |          | 50/? [00:00<00:00, 61.30it/s, v_num=8172]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 450: 'train_loss' reached 8.98384 (best 8.98384), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8172/checkpoints/epoch=8-step=450.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: |          | 50/? [00:00<00:00, 60.61it/s, v_num=8172]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 500: 'train_loss' reached 8.90680 (best 8.90680), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8172/checkpoints/epoch=9-step=500.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: |          | 50/? [00:00<00:00, 58.80it/s, v_num=8172]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 550: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: |          | 50/? [00:00<00:00, 58.52it/s, v_num=8172]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 600: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: |          | 50/? [00:00<00:00, 62.21it/s, v_num=8172]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 650: 'train_loss' reached 8.83085 (best 8.83085), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8172/checkpoints/epoch=12-step=650.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: |          | 50/? [00:00<00:00, 61.07it/s, v_num=8172]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 700: 'train_loss' reached 8.82542 (best 8.82542), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8172/checkpoints/epoch=13-step=700.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: |          | 50/? [00:00<00:00, 57.20it/s, v_num=8172]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 750: 'train_loss' reached 8.74685 (best 8.74685), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8172/checkpoints/epoch=14-step=750.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: |          | 50/? [00:00<00:00, 61.29it/s, v_num=8172]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 800: 'train_loss' reached 8.63272 (best 8.63272), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8172/checkpoints/epoch=15-step=800.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: |          | 50/? [00:00<00:00, 64.23it/s, v_num=8172]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 850: 'train_loss' reached 8.56289 (best 8.56289), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8172/checkpoints/epoch=16-step=850.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: |          | 50/? [00:00<00:00, 64.11it/s, v_num=8172]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 900: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: |          | 50/? [00:00<00:00, 64.64it/s, v_num=8172]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 950: 'train_loss' reached 8.52395 (best 8.52395), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8172/checkpoints/epoch=18-step=950.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: |          | 50/? [00:00<00:00, 65.50it/s, v_num=8172]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 1000: 'train_loss' reached 8.51469 (best 8.51469), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8172/checkpoints/epoch=19-step=1000.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: |          | 50/? [00:00<00:00, 65.82it/s, v_num=8172]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, global step 1050: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: |          | 50/? [00:00<00:00, 65.82it/s, v_num=8172]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, global step 1100: 'train_loss' reached 8.42555 (best 8.42555), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8172/checkpoints/epoch=21-step=1100.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: |          | 50/? [00:00<00:00, 64.07it/s, v_num=8172]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22, global step 1150: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: |          | 50/? [00:00<00:00, 66.41it/s, v_num=8172]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23, global step 1200: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: |          | 50/? [00:00<00:00, 64.16it/s, v_num=8172]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24, global step 1250: 'train_loss' reached 8.40839 (best 8.40839), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8172/checkpoints/epoch=24-step=1250.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: |          | 50/? [00:00<00:00, 62.53it/s, v_num=8172]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25, global step 1300: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: |          | 50/? [00:00<00:00, 59.11it/s, v_num=8172]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26, global step 1350: 'train_loss' reached 8.36022 (best 8.36022), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8172/checkpoints/epoch=26-step=1350.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: |          | 50/? [00:00<00:00, 58.97it/s, v_num=8172]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27, global step 1400: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: |          | 9/? [00:00<00:00, 57.67it/s, v_num=8172] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/fabric/utilities/cloud_io.py:56: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Forecasting batches: 100%|██████████| 1/1 [00:00<00:00, 13.34it/s]\n",
      "Ground truth: 100%|██████████| 1/1 [00:00<00:00, 4583.94it/s]\n",
      "/tmp/ipykernel_567778/2384952011.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"lag-llama.ckpt\", map_location=device)\n",
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/fabric/utilities/cloud_io.py:56: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Forecasting batches: 100%|██████████| 1/1 [00:00<00:00, 13.14it/s]\n",
      "Ground truth: 100%|██████████| 1/1 [00:00<00:00, 4002.20it/s]\n",
      "/tmp/ipykernel_567778/2384952011.py:131: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"lag-llama.ckpt\", map_location=device)\n",
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/fabric/utilities/cloud_io.py:56: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name          | Type               | Params\n",
      "-----------------------------------------------------\n",
      "0 | model         | LagLlamaModel      | 2.4 M \n",
      "1 | augmentations | ApplyAugmentations | 0     \n",
      "-----------------------------------------------------\n",
      "2.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 M     Total params\n",
      "9.797     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: |          | 50/? [00:00<00:00, 59.14it/s, v_num=8173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 50: 'train_loss' reached 10.31713 (best 10.31713), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8173/checkpoints/epoch=0-step=50.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: |          | 50/? [00:00<00:00, 60.05it/s, v_num=8173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 100: 'train_loss' reached 9.54966 (best 9.54966), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8173/checkpoints/epoch=1-step=100.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: |          | 50/? [00:00<00:00, 64.90it/s, v_num=8173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 150: 'train_loss' reached 9.53169 (best 9.53169), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8173/checkpoints/epoch=2-step=150.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: |          | 50/? [00:00<00:00, 64.77it/s, v_num=8173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 200: 'train_loss' reached 9.48139 (best 9.48139), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8173/checkpoints/epoch=3-step=200.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: |          | 50/? [00:00<00:00, 67.66it/s, v_num=8173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 250: 'train_loss' reached 9.05577 (best 9.05577), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8173/checkpoints/epoch=4-step=250.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: |          | 50/? [00:00<00:00, 66.19it/s, v_num=8173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 300: 'train_loss' reached 8.94937 (best 8.94937), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8173/checkpoints/epoch=5-step=300.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: |          | 50/? [00:00<00:00, 68.50it/s, v_num=8173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 350: 'train_loss' reached 8.78341 (best 8.78341), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8173/checkpoints/epoch=6-step=350.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: |          | 50/? [00:00<00:00, 70.64it/s, v_num=8173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 400: 'train_loss' reached 8.60605 (best 8.60605), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8173/checkpoints/epoch=7-step=400.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: |          | 50/? [00:00<00:00, 59.10it/s, v_num=8173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 450: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: |          | 50/? [00:00<00:00, 70.84it/s, v_num=8173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 500: 'train_loss' reached 8.38104 (best 8.38104), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8173/checkpoints/epoch=9-step=500.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: |          | 50/? [00:00<00:00, 70.40it/s, v_num=8173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 550: 'train_loss' reached 8.37770 (best 8.37770), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8173/checkpoints/epoch=10-step=550.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: |          | 50/? [00:00<00:00, 70.94it/s, v_num=8173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 600: 'train_loss' reached 8.22860 (best 8.22860), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8173/checkpoints/epoch=11-step=600.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: |          | 50/? [00:00<00:00, 70.47it/s, v_num=8173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 650: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: |          | 50/? [00:00<00:00, 69.57it/s, v_num=8173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 700: 'train_loss' reached 8.18494 (best 8.18494), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8173/checkpoints/epoch=13-step=700.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: |          | 50/? [00:00<00:00, 57.18it/s, v_num=8173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 750: 'train_loss' reached 8.04651 (best 8.04651), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8173/checkpoints/epoch=14-step=750.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: |          | 50/? [00:00<00:00, 64.34it/s, v_num=8173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 800: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: |          | 50/? [00:00<00:00, 59.07it/s, v_num=8173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 850: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: |          | 50/? [00:00<00:00, 60.73it/s, v_num=8173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 900: 'train_loss' reached 8.04527 (best 8.04527), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8173/checkpoints/epoch=17-step=900.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: |          | 50/? [00:00<00:00, 59.56it/s, v_num=8173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 950: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: |          | 50/? [00:00<00:00, 59.77it/s, v_num=8173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 1000: 'train_loss' reached 7.95625 (best 7.95625), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8173/checkpoints/epoch=19-step=1000.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: |          | 50/? [00:00<00:00, 59.60it/s, v_num=8173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, global step 1050: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: |          | 50/? [00:00<00:00, 59.78it/s, v_num=8173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, global step 1100: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: |          | 50/? [00:00<00:00, 60.15it/s, v_num=8173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22, global step 1150: 'train_loss' reached 7.91156 (best 7.91156), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8173/checkpoints/epoch=22-step=1150.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: |          | 50/? [00:00<00:00, 59.79it/s, v_num=8173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23, global step 1200: 'train_loss' reached 7.89525 (best 7.89525), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8173/checkpoints/epoch=23-step=1200.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: |          | 50/? [00:00<00:00, 62.65it/s, v_num=8173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24, global step 1250: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: |          | 50/? [00:00<00:00, 60.64it/s, v_num=8173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25, global step 1300: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: |          | 50/? [00:00<00:00, 62.66it/s, v_num=8173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26, global step 1350: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: |          | 50/? [00:00<00:00, 59.82it/s, v_num=8173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27, global step 1400: 'train_loss' reached 7.80978 (best 7.80978), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8173/checkpoints/epoch=27-step=1400.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: |          | 50/? [00:00<00:00, 65.53it/s, v_num=8173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28, global step 1450: 'train_loss' reached 7.76527 (best 7.76527), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8173/checkpoints/epoch=28-step=1450.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: |          | 50/? [00:00<00:00, 60.28it/s, v_num=8173]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29, global step 1500: 'train_loss' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: |          | 50/? [00:00<00:00, 59.52it/s, v_num=8173]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/fabric/utilities/cloud_io.py:56: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Forecasting batches: 100%|██████████| 1/1 [00:00<00:00, 13.61it/s]\n",
      "Ground truth: 100%|██████████| 1/1 [00:00<00:00, 4350.94it/s]\n",
      "/tmp/ipykernel_567778/2384952011.py:48: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"lag-llama.ckpt\", map_location=device)\n",
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/fabric/utilities/cloud_io.py:56: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Forecasting batches: 100%|██████████| 1/1 [00:00<00:00, 13.08it/s]\n",
      "Ground truth: 100%|██████████| 1/1 [00:00<00:00, 4240.95it/s]\n",
      "/tmp/ipykernel_567778/2384952011.py:131: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"lag-llama.ckpt\", map_location=device)\n",
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/fabric/utilities/cloud_io.py:56: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/user/miniconda3/envs/rapids_llama_new/lib/python3.9/site-packages/lightning/pytorch/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name          | Type               | Params\n",
      "-----------------------------------------------------\n",
      "0 | model         | LagLlamaModel      | 2.4 M \n",
      "1 | augmentations | ApplyAugmentations | 0     \n",
      "-----------------------------------------------------\n",
      "2.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 M     Total params\n",
      "9.797     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: |          | 50/? [00:00<00:00, 59.73it/s, v_num=8174]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 50: 'train_loss' reached 11.54605 (best 11.54605), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8174/checkpoints/epoch=0-step=50.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: |          | 50/? [00:00<00:00, 59.15it/s, v_num=8174]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 100: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: |          | 50/? [00:00<00:00, 58.90it/s, v_num=8174]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 150: 'train_loss' reached 11.46365 (best 11.46365), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8174/checkpoints/epoch=2-step=150.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: |          | 50/? [00:00<00:00, 58.08it/s, v_num=8174]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 200: 'train_loss' reached 11.38833 (best 11.38833), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8174/checkpoints/epoch=3-step=200.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: |          | 50/? [00:00<00:00, 58.10it/s, v_num=8174]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 250: 'train_loss' reached 11.38459 (best 11.38459), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8174/checkpoints/epoch=4-step=250.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: |          | 50/? [00:00<00:00, 59.47it/s, v_num=8174]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 300: 'train_loss' reached 11.27288 (best 11.27288), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8174/checkpoints/epoch=5-step=300.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: |          | 50/? [00:00<00:00, 59.47it/s, v_num=8174]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 350: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: |          | 50/? [00:00<00:00, 60.90it/s, v_num=8174]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 400: 'train_loss' reached 11.19297 (best 11.19297), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8174/checkpoints/epoch=7-step=400.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: |          | 50/? [00:00<00:00, 67.22it/s, v_num=8174]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 450: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: |          | 50/? [00:00<00:00, 66.43it/s, v_num=8174]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 500: 'train_loss' reached 10.33257 (best 10.33257), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8174/checkpoints/epoch=9-step=500.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: |          | 50/? [00:00<00:00, 57.07it/s, v_num=8174]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 550: 'train_loss' reached 10.01230 (best 10.01230), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8174/checkpoints/epoch=10-step=550.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: |          | 50/? [00:00<00:00, 56.99it/s, v_num=8174]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 600: 'train_loss' reached 9.87613 (best 9.87613), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8174/checkpoints/epoch=11-step=600.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: |          | 50/? [00:00<00:00, 58.90it/s, v_num=8174]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 650: 'train_loss' reached 9.87068 (best 9.87068), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8174/checkpoints/epoch=12-step=650.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: |          | 50/? [00:00<00:00, 61.97it/s, v_num=8174]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 700: 'train_loss' reached 9.85821 (best 9.85821), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8174/checkpoints/epoch=13-step=700.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: |          | 50/? [00:00<00:00, 61.75it/s, v_num=8174]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 750: 'train_loss' reached 9.71408 (best 9.71408), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8174/checkpoints/epoch=14-step=750.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: |          | 50/? [00:00<00:00, 57.86it/s, v_num=8174]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 800: 'train_loss' reached 9.62420 (best 9.62420), saving model to '/home/user/Área de trabalho/LAGLLAMA/lag-llama2-FT/lightning_logs/version_8174/checkpoints/epoch=15-step=800.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: |          | 50/? [00:00<00:00, 58.41it/s, v_num=8174]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 850: 'train_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: |          | 26/? [00:00<00:00, 59.55it/s, v_num=8174]"
     ]
    }
   ],
   "source": [
    "####### LagLlama Indiv ######\n",
    "\n",
    "from itertools import islice\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from tqdm.autonotebook import tqdm\n",
    "import torch\n",
    "from gluonts.evaluation import make_evaluation_predictions, Evaluator\n",
    "from gluonts.dataset.repository.datasets import get_dataset\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "from lag_llama.gluon.estimator import LagLlamaEstimator\n",
    "from gluonts.dataset.common import ListDataset\n",
    "import pandas as pd\n",
    "\n",
    "exp = 'Exp25_2024'\n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "products = sorted([name for name in os.listdir('../uf/') if os.path.isdir(os.path.join('../uf/', name))])\n",
    "\n",
    "def extract_estado(file_name):\n",
    "    parts = file_name.split('_')\n",
    "    estado = parts[1]\n",
    "    return estado\n",
    "\n",
    "def read_csv_files(folder_path):\n",
    "    estados = []\n",
    "    files = os.listdir(folder_path)\n",
    "    for file_name in files:\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, 'r', newline='') as csvfile:\n",
    "                reader = csv.reader(csvfile)\n",
    "                headers = next(reader)\n",
    "                estado = extract_estado(file_name)\n",
    "                estados.append(estado)\n",
    "                estados.sort()\n",
    "    return estados\n",
    "\n",
    "\n",
    "def get_lag_llama_predictions(dataset, prediction_length, context_length=48, num_samples=300, device=\"cuda\", batch_size=64, nonnegative_pred_samples=True): \n",
    "    ckpt = torch.load(\"lag-llama.ckpt\", map_location=device)\n",
    "    estimator_args = ckpt[\"hyper_parameters\"][\"model_kwargs\"]\n",
    "    estimator = LagLlamaEstimator(\n",
    "        ckpt_path=\"lag-llama.ckpt\",\n",
    "        prediction_length=prediction_length,\n",
    "        context_length=context_length,\n",
    "        input_size=estimator_args[\"input_size\"],\n",
    "        n_layer=estimator_args[\"n_layer\"],\n",
    "        n_embd_per_head=estimator_args[\"n_embd_per_head\"],\n",
    "        n_head=estimator_args[\"n_head\"],\n",
    "        scaling=estimator_args[\"scaling\"],\n",
    "        time_feat=estimator_args[\"time_feat\"],\n",
    "        nonnegative_pred_samples=nonnegative_pred_samples,\n",
    "        rope_scaling={\n",
    "            \"type\": \"linear\",\n",
    "            \"factor\": max(1.0, (context_length + prediction_length) / estimator_args[\"context_length\"]),\n",
    "        },\n",
    "        batch_size=batch_size,\n",
    "        num_parallel_samples=num_samples,\n",
    "    )\n",
    "    lightning_module = estimator.create_lightning_module()\n",
    "    transformation = estimator.create_transformation()\n",
    "    predictor = estimator.create_predictor(transformation, lightning_module)\n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=dataset,\n",
    "        predictor=predictor,\n",
    "        num_samples=num_samples\n",
    "    )\n",
    "    forecasts = list(tqdm(forecast_it, total=len(dataset), desc=\"Forecasting batches\"))\n",
    "    tss = list(tqdm(ts_it, total=len(dataset), desc=\"Ground truth\"))\n",
    "    return forecasts, tss\n",
    "\n",
    "for product in products:\n",
    "    folder_path = f'../../uf/{product}/'\n",
    "    estados = read_csv_files(folder_path)\n",
    "    for estado in estados:\n",
    "        dataset_own = pd.read_csv(f\"../uf/{product}/mensal_{estado}_{product}.csv\", header=0, sep=\";\")\n",
    "        target_values = dataset_own['m3']\n",
    "        start_date = pd.to_datetime(dataset_own['timestamp'].iloc[0], format='%Y%m')\n",
    "        prediction_length = 12\n",
    "        train_dataset = ListDataset(\n",
    "            [{\"start\": '1990-01', \"target\": target_values[:-prediction_length], \"item_id\": product}],\n",
    "            freq=\"M\"\n",
    "        )\n",
    "        test_dataset = ListDataset(\n",
    "            [{\"start\": '1990-01', \"target\": target_values, \"item_id\": product}],\n",
    "            freq=\"M\"\n",
    "        )\n",
    "        context_length = prediction_length*3\n",
    "        num_samples = 300\n",
    "        device = \"cuda\"\n",
    "        forecasts, tss = get_lag_llama_predictions(\n",
    "            test_dataset,\n",
    "            prediction_length=prediction_length,\n",
    "            num_samples=num_samples,\n",
    "            context_length=context_length,\n",
    "            device=device\n",
    "        )\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        date_formater = mdates.DateFormatter('%b, %d')\n",
    "        plt.rcParams.update({'font.size': 15})\n",
    "        for idx, (forecast, ts) in islice(enumerate(zip(forecasts, tss)), 9):\n",
    "            ax = plt.subplot(1, 1, idx+1)\n",
    "            plt.plot(ts[-4 * prediction_length:].to_timestamp(), label=\"target\", )\n",
    "            forecast.plot( color='g')\n",
    "            plt.xticks(rotation=60)\n",
    "            ax.xaxis.set_major_formatter(date_formater)\n",
    "            ax.set_title(forecast.item_id)\n",
    "        plt.gcf().tight_layout()\n",
    "        plt.legend()\n",
    "        plt.savefig(f'./{exp}/forecast_plots7_{product}_{estado}.png') \n",
    "        ckpt = torch.load(\"lag-llama.ckpt\", map_location=device)\n",
    "        estimator_args = ckpt[\"hyper_parameters\"][\"model_kwargs\"]\n",
    "        estimator = LagLlamaEstimator(\n",
    "                ckpt_path=\"lag-llama.ckpt\",\n",
    "                prediction_length=prediction_length,\n",
    "                context_length=context_length,\n",
    "                nonnegative_pred_samples=True,\n",
    "                aug_prob=0.5,\n",
    "                lr=5e-3,\n",
    "                input_size=estimator_args[\"input_size\"],\n",
    "                n_layer=estimator_args[\"n_layer\"],\n",
    "                n_embd_per_head=estimator_args[\"n_embd_per_head\"],\n",
    "                n_head=estimator_args[\"n_head\"],\n",
    "                time_feat=estimator_args[\"time_feat\"],\n",
    "                batch_size=64,\n",
    "                num_parallel_samples=num_samples,\n",
    "                trainer_kwargs = {\"max_epochs\": 30,},\n",
    "            )\n",
    "        predictor = estimator.train(train_dataset, cache_data=True)\n",
    "        forecast_it, ts_it = make_evaluation_predictions(\n",
    "            dataset=test_dataset,\n",
    "            predictor=predictor,\n",
    "            num_samples=num_samples\n",
    "        )\n",
    "        forecasts = list(tqdm(forecast_it, total=len(test_dataset), desc=\"Forecasting batches\"))\n",
    "        tss = list(tqdm(ts_it, total=len(test_dataset), desc=\"Ground truth\"))\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        date_formater = mdates.DateFormatter('%b, %d')\n",
    "        plt.rcParams.update({'font.size': 15})\n",
    "        for idx, (forecast, ts) in islice(enumerate(zip(forecasts, tss)), 9):\n",
    "            ax = plt.subplot(1, 1, idx+1)\n",
    "            plt.plot(ts[-4 * prediction_length:].to_timestamp(), label=\"target\", )\n",
    "            forecast.plot( color='g')\n",
    "            plt.xticks(rotation=60)\n",
    "            ax.xaxis.set_major_formatter(date_formater)\n",
    "            ax.set_title(forecast.item_id)\n",
    "        plt.gcf().tight_layout()\n",
    "        plt.legend()\n",
    "        plt.savefig(f'./{exp}/forecast_plots8_{product}_{estado}.png')\n",
    "        with open(f'./{exp}/LagLlama_{exp}.csv', 'a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            temp = forecast.samples.mean(axis=0)\n",
    "            values_list = temp.flatten().tolist()\n",
    "            writer.writerow([product, estado, 'LagLlama', values_list])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids_llama_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
